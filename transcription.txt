00:00:00 Hi I'm Dan jurafsky and Chris Manning and I are very happy to welcome you to our course on natural language processing this is not particularly exciting time to be working on natural language processing the vast amount of data on the web and social media have made it possible to build fantastic new applications let's look at one of them question answering you may know that IBM's Watson won the jeopardy challenge on February 16th 2011 I'm answering questions like William wilkinson's book inspired.
00:00:30 Souther's most famous novel and you may know that the answer is bram stoker who famously wrote Dracula.
00:01:00 Extractives information create a new calendar in tree and then populate the calendar with this kind of structured information with the event date start and end for calendar program in modern email and calendar programs are capable of doing this from text another application of this kind of information extraction involve sentiment analysis imagine that you're interested in cameras in your reading a lot of reviews of cameras on the web so here's a bunch of.
00:01:30 Reviews would like to automatically determine from the reviews that would people care about and cameras or particular attribute their buying a camera they wanna know if his goods umoor affordability or size and weight 20 automatically determine those attributes and then we'd like to automatically for any particular attribute determine how to review it felt about those attributes for example if you if you were said nice and compact carry that's a positive sentiment and here's another positive example but but I freeze like.
00:02:00 Lindsey is a negative sentiment so we'd like to automatically detect for each sentence with the sentiment is and then aggregate for each features for a safe resume for affordability so it might decide this camera reviews really like the flash but they weren't so happy about the ease of use we might measure the positive and negative sentiment bodychecked Butte and then aggregate those machine translation is another important new application and machine translation can be fully automatic so.
00:02:30 For example we might have a source sentence in Chinese and he was stanfords frezel MT system translating that into English but empty can also be used to help human translators so here we might have an Arabic text and the human translator translated into English might need some help from the empty system for example of a collection of possible next words that damn piece of them can build automatically and help the human translator let's look at the state of the art in which technology.
00:03:00 Specialties and subspecialties number of these problems are pretty close to solve so for example spam detection well it's very hard to completely to text spam email boxes we don't have a 99% spam and that's because spam detection is a relatively easy classification task I'm a couple of important component tasks part of speech tagging in named entity tagging will talk about those out later in the course and those work at pretty high accuracy.
00:00:00 7% accuracy in part of speech tagging and will you see how that's important for person in other tasks were making good progress not as commercial not as completely solved but there are systems out there that are that are being used so we talked about sentiment analysis the task of deciding thumbs up or thumbs down this sentence or product component technologies like word sense disambiguation deciding if we're talking about a rodent or a computer mouse when people talk about mouse is in a search then we'll talk.
00:00:30 Parsing which is a good enough now to be using lots of applications and machine translation usable on the web a number of applications however are still quite hard so for example at answering hard questions like how effective is this medicine in trying that disease by looking at the web I summarizing information we know is quite hard similarly what we made some progress on the siding that the sentence XYZ company acquired ABC company yesterday mean something similar to.
00:01:00 ABC's been taken over by XYZ the general problem of detecting the 2 phrases or sentences mean the same thing to paraphrase task still quite hard even harder is the task of summarization reading a number of let's a news article that sell the Dow Jones up or the S&P 500 is jumped and housing prices rose and aggregating that to give up user information like in summary the economies good and finally one of the hardest tasks in natural language.
00:01:30 Sing Cara not a complete human machine communication in dialogue so here's a simple example asking about what movie is playing when buy movie tickets and you can get applications but do that today but the general problem understanding everything to use are my task for and returning a sensable response is quite difficult why is natural language processing so difficult one example are the kinds of the ambiguity problems.
00:02:00 Call crash blossoms ambiguities any case where a surface for might have multiple interpretations crash blossom is the name for a kind of headline that has 2 meanings and then big booty causes a humorous interpretation so reading this first headline violinist linked to jail crash blossoms you might think that the main verb is linked in a violinist is being linked to what he's been linked to Japan Airlines crash blossoms will.
00:02:30 What is happening give a name to this phenomenon because the actual interpretation that the headline writer intended the main verbs blossoms who does the blossoming a violinist and this fact about being linked in jail crash was a modifier violin?
00:03:00 Asian is that the teachers striking strike is the verb and we have a teacher striking Idol kids another important kind of ambiguity is word sense ambiguity so when are third example red tape holds up new bridge is the writer intended hold up to mean something like delay call that since one of hold up but the time I'm using interpretation is the second sense of hold up with we might write down his to.
00:00:00 Port and now we get the interpretation that little red tape it supposed to be or cratic red tape is actually supporting the bridge and we can see lots of other kinds of'em ambiguities in the actual headlines now it turns out that it's not just amusing headlines that have any ambiguity is pervasive throughout natural language text let's look it up sensable nunobiki was looking headline from the New York times to the headline with short and to hear a bit and is.
00:00:30 Fed raises interest rates.
00:01:00 So this is called the phrase structure parts will talk about that later in the course for a structure.
00:01:30 That's the main verb in the sentence but interest somebody interest something and then that's something that gets interested is rate.
00:02:00 Reasonable interpretation we have to learn how to rule out in fact the sentence can get even more difficult this is the actual headline with something with longer so we had fed raise interest rates half a percent here we could imagine that rates is the verb and now we have what is reading fed raises interest interest in federal raises or rating half a percent so we might have a the pensie structurale do this so again interest.
00:02:30 Rates the razor what do the interesting in the fedesa motor fire reasons so weather with our free structure parse or dependency parsegh and even more so as we add more words with get more and more and big witty that have to be solved in order to build a parse for each sentence the format of the course you're gonna have nvidia quizzes and most Electro swing klouda little quiz there just to check basic understanding their simple multiple choice questions you can we take them if you got the wrong.
00:03:00 See one right now number of other things make natural language understanding difficult one of them is the nonstandard English that we only see in text like Twitter feeds where we have capitalization and unusual spelling of words and hashtags and user IDS and so on so all of our parsers and part of speech taggers that were gonna make you some more often trimdon very clean the newspaper text English but the actual English in there in the wild but I will cause a lot of problems will have a lot of sense.
00:00:00 Nation problems for example if we see that the string yor K dash NEW as part of New York New Haven how do we know the correct segmentation is New York and New Haven to the New York New Haven railroad and not something like York dash new this word here is not a word like in dash law we have to solve the second patient problem correctly we have problems with adiam and with some new words I haven't be seen.
00:00:30 Work and will also have problems with into the names like the movie a bug's life which has English words in it in so it's often difficult to know where the movie name starts and ends and it comes out very often in biology we have genes and proteins named with English words that asking actually understanding it's very difficult what tools do we need but we need knowledge about language knowledge about the world in a way to combine these knowledge sources so gently we do this is to use probabilistic models.
00:01:00 They are built from language data so for example if we see the word mezon in French were very likely to translate that has the word house in English the word friends were very unlikely to translate that as the general avocado and training these public models in general can be very hard but it turns out that we can do an approximate job of probably stick models with Ruff text features and will introduce those rough text features as we go so I'm going.
00:01:30 My class is teaching key theory and methods for statistical natural language processing will talk about the turbi algorithm naive bayes in Maxim classifiers bluejuice in grand language modeling in statistical parsing will talk about the inverted index in TF IDF and vector models of meaning that important information retrieval and will do this first practical robots real world applications will talk about information extraction about spelling correction about information retrieval.
00:02:00 The skills you need for the task you'll need simple linear algebra you should know what factor is I wanna make sure it says you should have some basic probability theory need to know how to program it either Java or python because the weekly programming assignment you have your choice of languages.
